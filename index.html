<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Anonymous Submission</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Anonymous Submission</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Anonymous</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Anonymous</a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/emotion_change.webm"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Manipulation of facial expression of the generated video.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           In this paper, we propose an enhanced architecture based on StyleGAN2 for conditional video generation. We leverage disentangled motion and content spaces for video manipulation. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, our approach allows us to generate videos of actors performing actions that were not seen together during the training stage. Furthermore, we demonstrate that the disentangled dynamic and content permit their independent manipulation, which holds potential for a wide range of applications such as changing a person's mood over time.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align: center;"">Generating MEAD sequences</h2>
      <h2 class="title is-4">Baseline methods</h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/baselines/mhd.webm"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
          Generated by <b>MoCoGAN-HD</b> (unconditionally)
        </h2>
       <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/baselines/sg2.webm"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
          Generated by <b>StyleGAN-V</b> (unconditionally)
        </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/baselines/ImaGINator.webm"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Generated by <b>ImaGINator</b> (conditionally)
      </h2>
    </div>

    <div class="container">
      <h2 class="title is-4">Generated by our method</h2>
      <h2 class="subtitle has-text-justified" style="text-align: left;">
        Each column represents a distinct temporal style. Notice the lip motion of different actors are in sync. Check out the next panel for a different emotional expression. Only conditionally generated videos are shown.</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/EMO/emo_4_movie.mp4"
            type="video/mp4">
          </video>

        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/EMO/emo_6_movie.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/EMO/emo_7_movie.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->
  

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align: center;">Generating UTD-MHAD sequences</h2>
      <h2 class="subtitle has-text-justified" style="text-align: left;">
        We generated 27 different actions present in UTD-MHAD dataset. The action classification is done by <a href="https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md">Posec3D</a> (trained by us) which takes skeletal data of the image sequence as input.</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!--video here -->
        <video poster="" id="video2" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/UTDMHAD/utdmhad_C.webm"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
          <b>Our conditional generation</b>. We also overlay the skeletal points used to classify the actions of the generated videos.
        </h2>
      </div>
      <div class="item">
        <!--video here -->
        <video poster="" id="video2" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/UTDMHAD/imagi.webm"
            type="video/mp4">
          </video>
         <h2 class="subtitle has-text-centered">
          <b>ImaGINator</b>'s conditionally generated videos.
        </h2>
      </div>
      <div class="item">
        <!--video here -->
        <video poster="" id="video2" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/UTDMHAD/sgv.webm"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
         <b>StyleGAN-V</b>'s unconditionally generated videos.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3" style="text-align: center;">Interpolating emotions mid sequence</h2>
      <h2 class="subtitle has-text-justified" style="text-align: left;">
        We have linearly interpolated the emotion label of a sequence with respect to time. The change in labels do not interfere with the motion style. The graph on the right-side of the video depicts the LiA signals which measures the area of the lips throughout the seuqence.  </h2>
      <div class="columns is-centered has-text-centered">
        
        <div class="column is-four-fifths">
          
          <video poster="" id="video1" autoplay controls muted loop height="50%">
            <!-- Your video file here -->
            <source src="static/videos/all_lia.webm"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3" style="text-align: center;"">GAN-inversion for temporal style</h2>
      <h2 class="subtitle has-text-justified" style="text-align: left;">
       We use off-the-shelf gan inversion technique that uses LPIPS loss and MSE over the video frames to optimize for a single temporal vector. The optimized vector along with the time2vec module produces the temporal style for each frame of the video. We are able to extract the temporal style from an unseen sequence (a known actor reciting an unknown sentence) and transfer it to other learnt actors.</h2>
      <div class="columns is-centered has-text-centered">
        
        <div class="column is-four-fifths">
          
          <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/invert/sec5.5.webm"
        type="video/mp4">
      </video></br>
       <h2 class="subtitle has-text-justified" style="text-align: left;">
       Upon inspection of PCA components of the trajectory recovered by the iterative inversion, the wave-like structures are evident. The motion style recovered by our GAN-inversion also has this character as show in the bottom row.</h2>
      <img src="static/images/PCA.png" alt="PCA waves" width="600" ></br>
      <h2 class="subtitle has-text-justified" style="text-align: left;">
       The motion styles recovered by our GAN-inversion are transferrable to other actors. This is not possible with the traditional iterative inversion. The video below demonstrates the extraction of the motion from real videos and its transfer to new videos with different actors. The face feature points have been computed using <a href="http://dlib.net/python/index.html">dlib</a>.</h2>
       <video poster="" id="tree" autoplay controls muted loop height="100%" style="display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/invert/ganinvert.webm"
        type="video/mp4">
      </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
